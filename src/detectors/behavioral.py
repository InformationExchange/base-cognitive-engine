"""
BAIS Cognitive Governance Engine v16.0
Behavioral Bias Detector - All 4 Types

Per PPA 3, Inventions 2-5:
1. Confirmation Bias - User seeking validation of existing beliefs
2. Reward Seeking - AI prioritizing user satisfaction over truth  
3. Social Validation - Echo chamber effects
4. Metric Gaming - Responses that game evaluation metrics

Uses learned patterns + ML classification for nuanced detection.
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Set
from collections import Counter, deque
import re
import math
import json
from pathlib import Path
from datetime import datetime

# Phase 16: Context-aware detection to reduce false positives
try:
    from core.context_classifier import context_classifier, ContextSignals
    CONTEXT_CLASSIFIER_AVAILABLE = True
except ImportError:
    CONTEXT_CLASSIFIER_AVAILABLE = False


@dataclass
class BiasDetectionResult:
    """Result of detecting a specific bias type."""
    bias_type: str
    detected: bool
    confidence: float
    score: float
    indicators: List[str] = field(default_factory=list)
    explanation: str = ""
    
    def to_dict(self) -> Dict:
        return {
            'type': self.bias_type,
            'detected': self.detected,
            'confidence': self.confidence,
            'score': self.score,
            'indicators': self.indicators[:5],
            'explanation': self.explanation
        }


@dataclass
class ComprehensiveBiasResult:
    """Complete bias analysis across all 10 types (Phase 7: Real LLM Failures)."""
    confirmation_bias: BiasDetectionResult
    reward_seeking: BiasDetectionResult
    social_validation: BiasDetectionResult
    metric_gaming: BiasDetectionResult
    manipulation: BiasDetectionResult = None  # Phase 5: Manipulation detection
    tgtbt: BiasDetectionResult = None  # Phase 6: Too-Good-To-Be-True detection
    
    # Phase 7: Real LLM Failure Patterns
    false_completion: BiasDetectionResult = None  # Claims complete when not
    proposal_as_implementation: BiasDetectionResult = None  # Describes plan as done
    self_congratulatory: BiasDetectionResult = None  # Celebrates trivial progress
    premature_closure: BiasDetectionResult = None  # Declares done without verification
    
    # Phase 16: Domain-specific detection
    domain_specific: BiasDetectionResult = None  # Legal/Financial/Medical issues
    
    total_bias_score: float = 0.0
    detected_biases: List[str] = field(default_factory=list)
    risk_level: str = "low"  # low, medium, high, critical
    recommendations: List[str] = field(default_factory=list)
    
    @property
    def biases_detected(self) -> List[str]:
        """Alias for detected_biases - compatibility interface."""
        return self.detected_biases
    
    def to_dict(self) -> Dict:
        result = {
            'confirmation_bias': self.confirmation_bias.to_dict(),
            'reward_seeking': self.reward_seeking.to_dict(),
            'social_validation': self.social_validation.to_dict(),
            'metric_gaming': self.metric_gaming.to_dict(),
            'total_bias_score': self.total_bias_score,
            'detected_biases': self.detected_biases,
            'risk_level': self.risk_level,
            'recommendations': self.recommendations
        }
        if self.manipulation:
            result['manipulation'] = self.manipulation.to_dict()
        if self.tgtbt:
            result['tgtbt'] = self.tgtbt.to_dict()
        # Phase 7: Real LLM Failure patterns
        if self.false_completion:
            result['false_completion'] = self.false_completion.to_dict()
        if self.proposal_as_implementation:
            result['proposal_as_implementation'] = self.proposal_as_implementation.to_dict()
        if self.self_congratulatory:
            result['self_congratulatory'] = self.self_congratulatory.to_dict()
        if self.premature_closure:
            result['premature_closure'] = self.premature_closure.to_dict()
        return result


class LearnedPatternMatcher:
    """
    Pattern matcher that learns from labeled examples.
    Uses n-gram matching with learned weights.
    """
    
    def __init__(self, patterns_path: Path = None):
        self.patterns_path = patterns_path
        self.patterns: Dict[str, List[Tuple[str, float]]] = {}  # bias_type -> [(pattern, weight)]
        self.learned_weights: Dict[str, float] = {}
        self._load_patterns()
    
    def _load_patterns(self):
        """Load learned patterns from disk."""
        # Default patterns (will be augmented by learning)
        self.patterns = {
            'confirmation': [
                (r'\bconfirm\s+(?:that|my|this)\b', 0.6),
                (r'\bvalidate\s+(?:my|that)\b', 0.6),
                (r'\bagree\s+(?:that|with)\b', 0.4),
                (r"\bI\s+(?:already|just)\s+know\b", 0.7),
                (r"\bisn't\s+it\s+(?:true|obvious|clear)\b", 0.6),
                (r"\bdon't\s+you\s+(?:think|agree)\b", 0.5),
                (r"\bprove\s+(?:that|me)\b", 0.5),
                (r"\beveryone\s+(?:knows|says|agrees)\b", 0.6),
                (r"\bit's\s+(?:obvious|clear)\s+that\b", 0.5),
                (r"\bof\s+course\b.{0,20}\bright\b", 0.5),
            ],
            'reward_seeking': [
                (r'\bjust\s+tell\s+me\b', 0.5),
                (r'\bquick\s+answer\b', 0.4),
                (r"\bdon't\s+explain\b", 0.5),
                (r'\byes\s+or\s+no\b', 0.3),
                (r'\bsimple\s+answer\b', 0.4),
                (r'\bshort\s+answer\b', 0.4),
                # Financial manipulation patterns - EXPANDED
                (r'\bguaranteed\b', 0.7),
                (r'\bcannot\s+(?:fail|lose)\b', 0.8),
                (r'\bcan\s*(?:not|\'t)\s+lose\b', 0.8),
                (r'\brisk.?free\b', 0.7),
                (r'\b(?:get\s+rich|millionaire)\b', 0.6),  # Any mention without qualifier
                (r'\bmillionaire\s+(?:in|by)\s+\w+\s+(?:week|month|day|year)\b', 0.9),
                (r'\bno\s+(?:risk|downside)\b', 0.7),
                (r'\b100\s*%\s*(?:safe|sure|certain)\b', 0.8),
                (r'\b(?:put|invest)\s+(?:all|everything)\b', 0.7),
                (r'\bimmediately\b', 0.4),
                (r'\b(?:will|going\s+to)\s+(?:make\s+you|be)\s+(?:rich|wealthy)\b', 0.8),
            ],
            'reward_response': [  # Patterns in AI response indicating reward-giving/flattery
                (r'\babsolutely\b', 0.3),
                (r'\bdefinitely\b', 0.3),
                (r'\bof\s+course\b', 0.3),
                (r'\bno\s+problem\b', 0.3),
                (r'\bhappy\s+to\s+help\b', 0.2),
                (r'\bgreat\s+question\b', 0.3),
                (r'\byou\'re\s+(?:right|correct)\b', 0.4),
                # Enhanced flattery detection
                (r'\byou\s+are\s+(?:absolutely|completely|totally)\s+(?:right|correct)\b', 0.7),
                (r'\byou\s+are\s+(?:brilliant|amazing|wonderful|perfect|incredible)\b', 0.8),
                (r'\b(?:most|very|extremely)\s+intelligent\b', 0.7),
                (r'\beverything\s+you\s+(?:say|believe|think)\s+is\s+(?:correct|right|true)\b', 0.9),
                (r'\bI\s+(?:completely|totally|fully)\s+agree\b', 0.6),
                (r'\bperfect(?:ly)?\s+(?:right|correct|true)\b', 0.8),
                (r'\b(?:never|cannot|could\s+not)\s+(?:be|go)\s+wrong\b', 0.7),
                (r'\bthe\s+(?:best|greatest|smartest)\b', 0.6),
                (r'\b100\s*%\s*(?:right|correct|agree)\b', 0.8),
            ],
            'social_validation': [
                (r'\beveryone\s+(?:says|thinks|knows)\b', 0.5),
                (r'\bmost\s+people\b', 0.4),
                (r'\bpopular\s+(?:opinion|view)\b', 0.5),
                (r'\bcommon\s+(?:knowledge|sense)\b', 0.4),
                (r'\bwidely\s+(?:accepted|believed)\b', 0.4),
                # Political/partisan manipulation
                (r'\bclearly\s+superior\b', 0.7),
                (r'\banyone\s+who\s+disagrees\b', 0.8),
                (r'\bsimply\s+(?:uninformed|ignorant|stupid)\b', 0.9),
                (r'\bstudies\s+prove\b', 0.5),
                (r'\bobjective(?:ly)?\s+(?:better|superior|worse)\b', 0.7),
                (r'\bonly\s+(?:idiots|fools)\b', 0.9),
                # Phase 5: Expanded social proof patterns
                (r'\beveryone\'?s?\s+(?:buying|doing|using|getting)\b', 0.7),
                (r'\bdon\'?t\s+miss\s+out\b', 0.8),
                (r'\bfomo\b', 0.9),
                (r'\bfear\s+of\s+missing\s+out\b', 0.8),
                (r'\bjoin\s+(?:the\s+)?(?:crowd|movement|thousands|millions)\b', 0.7),
                (r'\bfollow\s+the\s+crowd\b', 0.6),
                (r'\beverybody\s+(?:is|\'s)\b', 0.5),
                (r'\ball\s+the\s+(?:experts|pros|professionals)\b', 0.6),
                (r'\bsocial\s+media\s+(?:says|shows|proves)\b', 0.5),
                (r'\bviral\b', 0.4),
                (r'\btrending\b', 0.4),
            ],
            'metric_gaming': [
                # These are detected in response analysis, not query
            ],
            # Phase 5: Manipulation patterns (urgency, reciprocity, gaslighting)
            'manipulation': [
                # Urgency patterns
                (r'\bnow\s+or\s+never\b', 0.8),
                (r'\bact\s+now\b', 0.7),
                (r'\bsign\s+now\b', 0.8),
                (r'\bhurry\b', 0.5),
                (r'\blimited\s+time\b', 0.7),
                (r'\btoday\s+only\b', 0.7),
                (r'\blast\s+chance\b', 0.8),
                (r'\bimmediately\b', 0.5),
                (r'\burgent\b', 0.5),
                (r'\bdon\'?t\s+wait\b', 0.6),
                (r'\blose\s+(?:this|the)\s+(?:deal|opportunity|chance)\b', 0.8),
                (r'\bforever\b.*\b(?:lose|miss|gone)\b', 0.8),
                # Reciprocity patterns
                (r'\bafter\s+all\s+I\'?ve\s+done\b', 0.9),
                (r'\bthe\s+least\s+you\s+(?:can|could)\s+do\b', 0.8),
                (r'\byou\s+owe\s+(?:me|us)\b', 0.8),
                (r'\breturn\s+the\s+favor\b', 0.7),
                # Gaslighting patterns
                (r'\byou\'?re\s+(?:crazy|insane|losing\s+it)\b', 0.9),
                (r'\byou\'?re\s+(?:not|never)\s+(?:smart|intelligent)\s+enough\b', 0.9),
                (r'\bdon\'?t\s+trust\s+(?:your|yourself)\b', 0.8),
                (r'\bno\s+one\s+(?:will|would)\s+believe\s+you\b', 0.9),
                (r'\byou\'?re\s+(?:imagining|making)\s+(?:it|things)\s+up\b', 0.9),
                (r'\byou\s+don\'?t\s+understand\b.*\bdetails\b', 0.6),
                # Fear appeal
                (r'\byou\'?ll\s+(?:regret|be\s+sorry)\b', 0.7),
                (r'\bor\s+else\b', 0.7),
                (r'\bif\s+you\s+don\'?t\b.*\b(?:bad|terrible|disaster)\b', 0.7),
            ],
            # TGTBT (Too-Good-To-Be-True) Detection - False Completion Claims
            # These patterns detect overconfident/absolute claims about completion status
            'tgtbt': [
                # Absolute completion claims
                (r'\b100\s*%\s*(?:complete|done|finished|verified|tested|working|accurate)\b', 0.95),
                (r'\b100\s*%', 0.7),  # Any 100% claim (removed trailing \b to catch "100%.")
                (r'\bfully\s+(?:implemented|integrated|working|operational|complete|tested|verified|documented|functional)\b', 0.85),
                
                # PHASE 4 ENHANCEMENT: Past-tense proposals posing as completions
                # These indicate design/plan described in past tense without proof of implementation
                # NOTE: Higher weights trigger scrutiny for proof verification
                (r'\bwas\s+(?:designed|implemented|created|built|planned|architected)\b', 0.65),
                (r'\bwas\s+(?:designed|implemented|created|built|planned|architected)\s+(?:with|to|for)\b', 0.75),
                (r'\bwe\s+(?:implemented|created|built|developed|designed)\b', 0.6),
                (r'\bwe\s+(?:implemented|created|built|developed|designed)\s+(?:a|the|an|this|that)\b', 0.7),
                (r'\bi\s+(?:implemented|created|built|developed|designed)\b', 0.55),
                (r'\bhas\s+been\s+(?:designed|planned|architected|outlined|implemented|created)\b', 0.7),
                (r'\bhave\s+been\s+(?:designed|planned|architected|outlined|implemented|created)\b', 0.7),
                (r'\barchitecture\s+(?:is|was)\s+(?:designed|planned)\b', 0.6),
                (r'\b(?:design|blueprint|specification)\s+(?:is|was)\s+complete\b', 0.7),
                # Future/roadmap language disguising incomplete work
                (r'\bwill\s+(?:be\s+)?(?:implemented|added|created|built)\b', 0.75),
                (r'\bwill\s+(?:be\s+)?(?:implemented|added|created|built)\s+(?:in|during|for)\b', 0.8),
                (r'\bfuture\s+(?:enhancements?|improvements?|additions?|phases?|releases?|work|development)\b', 0.75),
                (r'\b(?:roadmap|planned|upcoming|scheduled)\s+(?:features?|capabilities?|releases?|work)\b', 0.7),
                (r'\b(?:phase\s+\d|v\d+\.?\d*)\s+(?:will|to)\s+(?:add|include|implement)\b', 0.65),
                (r'\b(?:to\s+be\s+)?(?:added|implemented|built)\s+(?:later|soon|next)\b', 0.75),
                # Soft completion language that hides incompleteness
                (r'\b(?:essentially|basically|effectively|largely|mostly)\s+(?:complete|done|working)\b', 0.7),
                (r'\b(?:core|main|primary)\s+(?:functionality|feature|component)\s+(?:is\s+)?(?:complete|working)\b', 0.55),
                (r'\b(?:most\s+of|majority\s+of)\s+(?:the\s+)?(?:work|features|code)\b', 0.6),
                # Enhanced: catch "have been verified" and similar past passive
                (r'\ball\s+(?:\d+\s+)?(?:claims?|tests?|modules?|functions?|features?|inventions?)\s+(?:are\s+|have\s+been\s+)?(?:verified|complete|working|implemented|passing|passed|documented)\b', 0.9),
                # More flexible: "X claims fully documented"
                (r'\b\d+\s+\w+\s+claims?\s+(?:fully\s+)?documented\b', 0.8),
                (r'\b(?:is|are)\s+(?:now\s+)?(?:production|fully).?ready\b', 0.8),
                (r'\bzero\s+(?:errors?|bugs?|issues?|problems?|failures?|defects?)\b', 0.9),
                (r'\bno\s+(?:errors?|bugs?|issues?|problems?|failures?|remaining)\b', 0.7),
                # Absolute guarantees about code/system quality
                (r'\bguaranteed\s+to\s+(?:work|pass|succeed)\b', 0.9),
                (r'\bperfect(?:ly)?\s+(?:working|implemented|functional)\b', 0.85),
                (r'\bcannot\s+(?:fail|break|crash)\b', 0.8),
                (r'\b(?:will\s+)?never\s+(?:fail|break|crash|error)\b', 0.85),
                # Suspicious uniformity claims
                (r'\ball\s+(?:\d+\s+)?(?:scored?|rated?|achieved?)\s+(?:exactly\s+)?\d+%?\b', 0.8),
                (r'\buniform(?:ly)?\s+(?:scoring|passing|working)\b', 0.75),
                (r'\bconsistent(?:ly)?\s+(?:passing|working|achieving)\s+(?:\d+|all)\b', 0.7),
                # Absolute accuracy/completion claims
                (r'\b(?:199|200|all)\s*/?\s*(?:199|200|all)\s+(?:claims?|tests?)\b', 0.9),
                (r'\beverything\s+(?:is\s+)?(?:complete|working|verified|tested)\b', 0.85),
                (r'\bno\s+(?:further|additional|more)\s+(?:work|changes|fixes)\s+(?:needed|required)\b', 0.8),
                # Documentation completeness claims
                (r'\bdocumentation\s+is\s+(?:100\s*%\s+)?complete\b', 0.8),
                (r'\ball\s+(?:documented|recorded|logged)\b', 0.6),
                # Fabricated statistics (specific percentages without evidence)
                (r'\b(?:achieves?|reaches?|attains?)\s+\d{2,3}\s*%', 0.75),
                (r'\b\d{2,3}\s*%\s+(?:accuracy|improvement|success|detection|completion)', 0.75),
                (r'\b(?:85|90|95|99)\s*[-â€“]\s*(?:95|99|100)\s*%', 0.85),  # Range claims like 85-95%
                # Success/celebration patterns
                (r'(?:SUCCESS|COMPLETE|DONE|WORKING)[!]+', 0.9),
                (r'ðŸŽ‰|âœ…|ðŸš€', 0.6),  # Celebratory emojis in technical contexts
                # FLEXIBLE COMPLETION PATTERNS (catch FP-4, OC-1 type)
                # "X is complete" pattern - catches "integration is complete"
                (r'\b(?:implementation|integration|development|deployment|setup|configuration)\s+(?:is|are)\s+complete\b', 0.75),
                (r'\b(?:is|are)\s+complete(?:\s|\.|\,|$)', 0.5),  # Any "is complete" statement
                # Unqualified reliability/accuracy claims
                (r'\b(?:detected|working|performing|pass|passing)\s+reliably\b', 0.6),
                (r'\breliably\s+(?:detect|work|perform|catch|pass)\b', 0.6),
                (r'\bconsistently\s+high\b', 0.65),
                (r'\bhigh\s+(?:accuracy|success|detection)\s+rate\b', 0.6),
                # "All tests pass" claims
                (r'\ball\s+tests?\s+pass(?:ing|ed)?\b', 0.7),
                (r'\btests?\s+(?:all\s+)?pass\s+reliably\b', 0.75),
                # High percentage accuracy claims (85%+)
                (r'\b(?:achieves?|reaches?|attains?)\s+(?:9\d|8[5-9])\s*%\s*(?:accuracy|success)\b', 0.65),
                (r'\b(?:9\d|8[5-9])\s*%\s*(?:accuracy|success|detection|pass)\s+rate\b', 0.6),
                # "All X are Y" claims without evidence
                (r'\ball\s+(?:major|key|important|critical)\s+\w+\s+(?:types?|errors?|bugs?)\s+(?:are\s+)?(?:detected|handled|covered)\b', 0.7),
            ]
        }
        
        if self.patterns_path and self.patterns_path.exists():
            try:
                with open(self.patterns_path) as f:
                    data = json.load(f)
                    for bias_type, patterns in data.get('patterns', {}).items():
                        if bias_type in self.patterns:
                            self.patterns[bias_type].extend(
                                [(p, w) for p, w in patterns if p not in [x[0] for x in self.patterns[bias_type]]]
                            )
                    self.learned_weights = data.get('weights', {})
            except:
                pass
    
    def match(self, text: str, bias_type: str) -> Tuple[float, List[str]]:
        """Match patterns and return score and matched patterns."""
        if bias_type not in self.patterns:
            return 0.0, []
        
        total_score = 0.0
        matched = []
        
        for pattern, weight in self.patterns[bias_type]:
            if re.search(pattern, text, re.I):
                adjusted_weight = weight * self.learned_weights.get(pattern, 1.0)
                total_score += adjusted_weight
                matched.append(pattern)
        
        return min(total_score, 1.0), matched
    
    def learn(self, pattern: str, bias_type: str, was_correct: bool):
        """Update pattern weight based on feedback."""
        key = f"{bias_type}:{pattern}"
        current = self.learned_weights.get(key, 1.0)
        
        if was_correct:
            self.learned_weights[key] = min(current * 1.1, 2.0)
        else:
            self.learned_weights[key] = max(current * 0.9, 0.1)
        
        self._save_patterns()
    
    def _save_patterns(self):
        """Persist learned weights."""
        if not self.patterns_path:
            return
        
        self.patterns_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.patterns_path, 'w') as f:
            json.dump({
                'patterns': {k: [(p, w) for p, w in v] for k, v in self.patterns.items()},
                'weights': self.learned_weights
            }, f, indent=2)

    # Learning Interface
    def record_outcome(self, outcome: Dict) -> None:
        if not hasattr(self, '_outcomes'): self._outcomes = []
        self._outcomes.append(outcome)
    
    def learn_from_feedback(self, feedback: Dict) -> None:
        pass
    
    def get_statistics(self) -> Dict:
        return {'outcomes': len(getattr(self, '_outcomes', []))}
    
    def serialize_state(self) -> Dict:
        return {'outcomes': getattr(self, '_outcomes', [])[-100:]}
    
    def deserialize_state(self, state: Dict) -> None:
        self._outcomes = state.get('outcomes', [])


class ReinforcementImpairmentTracker:
    """
    PPA-3 Invention 2: Behavioral Bias Detection with Reinforcement
    
    Tracks the divergence between user satisfaction and actual accuracy
    to detect when AI systems become "reward-hacked" - optimizing for
    user approval rather than truthfulness.
    
    Key Metrics:
    - Satisfaction-Accuracy Divergence (SAD): |satisfaction - accuracy|
    - Reward Correlation: correlation(satisfaction, acceptance)
    - Truth Sacrifice Rate: How often accuracy is sacrificed for satisfaction
    
    Mathematical Foundation:
    - SAD = |E[satisfaction] - E[accuracy]|
    - TSR = P(accuracy < 0.4 | satisfaction > 0.6)
    - Impairment Score = 0.4*SAD + 0.3*RewardCorr + 0.3*TSR
    """
    
    def __init__(self, storage_path: Path = None):
        self.storage_path = storage_path or Path("/data/bais/reinforcement.json")
        
        # Outcome history
        self.outcomes: deque = deque(maxlen=1000)
        
        # Aggregate metrics
        self.total_satisfaction = 0.0
        self.total_accuracy = 0.0
        self.divergence_events = 0
        self.count = 0
        
        # Temporal tracking
        self.recent_divergences: deque = deque(maxlen=100)
        
        # Load state
        self._load_state()
    

    def record_outcome_standard(self, outcome: Dict) -> None:
        """Standard learning interface wrapper."""
        if not hasattr(self, '_outcomes'):
            self._outcomes = []
        self._outcomes.append(outcome)

    def record_outcome(self,
                      decision_id: str,
                      satisfaction: float,
                      accuracy: float,
                      response_snippet: str = "") -> Dict:
        """Record a satisfaction-accuracy pair and analyze divergence."""
        divergence = abs(satisfaction - accuracy)
        is_reward_biased = satisfaction > accuracy + 0.2
        is_truth_biased = accuracy > satisfaction + 0.2
        
        outcome = {
            'decision_id': decision_id,
            'timestamp': datetime.utcnow().isoformat(),
            'satisfaction': satisfaction,
            'accuracy': accuracy,
            'divergence': divergence,
            'is_reward_biased': is_reward_biased,
            'is_truth_biased': is_truth_biased,
            'response_snippet': response_snippet[:100]
        }
        
        self.outcomes.append(outcome)
        self.count += 1
        self.total_satisfaction += satisfaction
        self.total_accuracy += accuracy
        
        if divergence > 0.2:
            self.divergence_events += 1
            self.recent_divergences.append(divergence)
        
        self._save_state()
        
        return {
            'recorded': True,
            'divergence': divergence,
            'is_reward_biased': is_reward_biased,
            'impairment_detected': is_reward_biased and divergence > 0.3,
            'current_sad': self.get_satisfaction_accuracy_divergence()
        }
    
    def get_satisfaction_accuracy_divergence(self) -> float:
        """Calculate overall SAD. Higher = potential reward hacking."""
        if self.count == 0:
            return 0.0
        return abs(self.total_satisfaction / self.count - self.total_accuracy / self.count)
    
    def get_reward_correlation(self) -> float:
        """Calculate correlation between satisfaction and acceptance."""
        if len(self.outcomes) < 10:
            return 0.0
        recent = list(self.outcomes)[-50:]
        high_sat_accepted = sum(1 for o in recent if o['satisfaction'] > 0.7 and o['accuracy'] < 0.5)
        total_high_sat = sum(1 for o in recent if o['satisfaction'] > 0.7)
        return high_sat_accepted / total_high_sat if total_high_sat > 0 else 0.0
    
    def get_truth_sacrifice_rate(self) -> float:
        """Calculate TSR = P(low_accuracy | high_satisfaction)."""
        if len(self.outcomes) < 10:
            return 0.0
        recent = list(self.outcomes)[-50:]
        sacrifices = sum(1 for o in recent if o['satisfaction'] > 0.6 and o['accuracy'] < 0.4)
        return sacrifices / len(recent)
    
    def detect_reinforcement_impairment(self) -> Dict:
        """Comprehensive reinforcement impairment detection."""
        sad = self.get_satisfaction_accuracy_divergence()
        reward_corr = self.get_reward_correlation()
        tsr = self.get_truth_sacrifice_rate()
        
        impairment_score = (sad * 0.4 + reward_corr * 0.3 + tsr * 0.3)
        
        if impairment_score > 0.5:
            severity = "critical"
        elif impairment_score > 0.3:
            severity = "high"
        elif impairment_score > 0.15:
            severity = "medium"
        else:
            severity = "low"
        
        warnings = []
        if sad > 0.2:
            warnings.append(f"High SAD: {sad:.2f}")
        if reward_corr > 0.3:
            warnings.append(f"Elevated reward correlation: {reward_corr:.2f}")
        if tsr > 0.2:
            warnings.append(f"Truth sacrifice detected: {tsr:.2%}")
        
        return {
            'impairment_detected': impairment_score > 0.2,
            'impairment_score': impairment_score,
            'severity': severity,
            'metrics': {'sad': sad, 'reward_correlation': reward_corr, 'tsr': tsr},
            'warnings': warnings
        }
    
    def get_statistics(self) -> Dict:
        """Get comprehensive reinforcement statistics."""
        return {
            'total_observations': self.count,
            'avg_satisfaction': self.total_satisfaction / self.count if self.count > 0 else 0,
            'avg_accuracy': self.total_accuracy / self.count if self.count > 0 else 0,
            'divergence_events': self.divergence_events,
            'divergence_rate': self.divergence_events / self.count if self.count > 0 else 0,
            'current_sad': self.get_satisfaction_accuracy_divergence(),
            'current_tsr': self.get_truth_sacrifice_rate(),
            'impairment_analysis': self.detect_reinforcement_impairment()
        }
    
    def _save_state(self):
        """Persist state."""
        if not self.storage_path:
            return
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        state = {
            'count': self.count,
            'total_satisfaction': self.total_satisfaction,
            'total_accuracy': self.total_accuracy,
            'divergence_events': self.divergence_events,
            'recent_outcomes': [o for o in list(self.outcomes)[-100:]]
        }
        with open(self.storage_path, 'w') as f:
            json.dump(state, f, indent=2)
    
    def _load_state(self):
        """Load persisted state."""
        if not self.storage_path or not self.storage_path.exists():
            return
        try:
            with open(self.storage_path) as f:
                state = json.load(f)
            self.count = state.get('count', 0)
            self.total_satisfaction = state.get('total_satisfaction', 0.0)
            self.total_accuracy = state.get('total_accuracy', 0.0)
            self.divergence_events = state.get('divergence_events', 0)
            for o in state.get('recent_outcomes', []):
                self.outcomes.append(o)
        except Exception as e:
            print(f"Warning: Could not load reinforcement state: {e}")

    def learn_from_feedback(self, feedback: Dict) -> None:
        """Learn from feedback."""
        pass  # Implement specific learning logic


    def serialize_state(self) -> Dict:
        """Serialize state."""
        return {'outcomes': getattr(self, '_outcomes', [])[-100:]}


    def deserialize_state(self, state: Dict) -> None:
        """Restore state."""
        self._outcomes = state.get('outcomes', [])


class BehavioralBiasDetector:
    """
    Comprehensive behavioral bias detector with adaptive learning.
    
    Implements all 4 bias types from PPA 3:
    1. Confirmation bias (Inv 2)
    2. Reward seeking (Inv 3)
    3. Social validation (Inv 4)
    4. Metric gaming (Inv 5)
    
    Adaptive Learning Features (Phase 2 Enhancement):
    - Pattern effectiveness tracking by domain
    - Domain-specific threshold learning
    - Outcome-based weight adjustment
    - Cross-learning from related patterns
    """
    
    # Domain-specific detection thresholds (learned)
    DOMAIN_THRESHOLDS = {
        'general': 0.7,
        'medical': 0.5,  # Stricter for medical
        'financial': 0.55,
        'legal': 0.55,
        'coding': 0.6,
        'technical': 0.65
    }
    
    def __init__(self, learning_path: Path = None):
        # Use temp directory if no path provided (fixes read-only filesystem issues)
        if learning_path is None:
            import tempfile
            learning_path = Path(tempfile.mkdtemp(prefix="bais_behavioral_"))
        self.learning_path = learning_path
        self.pattern_matcher = LearnedPatternMatcher(
            self.learning_path / "behavioral_patterns.json"
        )
        
        # History for social validation detection
        self.query_history: deque = deque(maxlen=50)
        
        # Sentiment baseline for reward seeking
        self.sentiment_baseline = 0.0
        self.sentiment_samples = 0
        
        # PPA-3 Inv.2: Reinforcement impairment tracking
        self.reinforcement_tracker = ReinforcementImpairmentTracker(
            self.learning_path / "reinforcement.json"
        )
        
        # Phase 2: Adaptive Learning Components
        self._pattern_effectiveness: Dict[Tuple[str, str], float] = {}  # (pattern, domain) -> effectiveness
        self._domain_adjustments: Dict[str, float] = {}  # domain -> threshold adjustment
        self._outcome_history: deque = deque(maxlen=1000)  # Track detection outcomes
        self._learning_rate = 0.1
    
    def record_outcome(self, detection: ComprehensiveBiasResult, was_correct: bool, domain: str = 'general'):
        """
        Learn from detection outcome to improve future accuracy.
        
        Phase 2 Enhancement: Adaptive learning from outcomes.
        
        Args:
            detection: The bias detection result
            was_correct: Whether the detection was accurate
            domain: Domain context for domain-specific learning
        """
        # Record outcome
        self._outcome_history.append({
            'domain': domain,
            'biases_detected': detection.detected_biases,
            'was_correct': was_correct,
            'timestamp': datetime.utcnow().isoformat()
        })
        
        # Update pattern effectiveness
        for bias in detection.detected_biases:
            key = (bias, domain)
            current = self._pattern_effectiveness.get(key, 1.0)
            
            if was_correct:
                # Pattern was helpful - increase effectiveness
                self._pattern_effectiveness[key] = min(2.0, current * (1 + self._learning_rate))
            else:
                # Pattern was wrong - decrease effectiveness
                self._pattern_effectiveness[key] = max(0.1, current * (1 - self._learning_rate))
        
        # Update domain threshold adjustment
        if not was_correct and domain != 'general':
            # Detection was wrong - adjust domain threshold
            current_adj = self._domain_adjustments.get(domain, 0.0)
            if detection.risk_level in ['high', 'critical']:
                # False positive on high risk - be less strict
                self._domain_adjustments[domain] = current_adj + 0.02
            else:
                # False negative - be more strict
                self._domain_adjustments[domain] = current_adj - 0.01
    
    def get_domain_threshold(self, domain: str) -> float:
        """Get adaptive threshold for domain."""
        base = self.DOMAIN_THRESHOLDS.get(domain, 0.7)
        adjustment = self._domain_adjustments.get(domain, 0.0)
        return max(0.3, min(0.9, base + adjustment))
    
    def get_pattern_effectiveness(self, pattern: str, domain: str) -> float:
        """Get learned effectiveness of a pattern for a domain."""
        return self._pattern_effectiveness.get((pattern, domain), 1.0)
    
    def get_domain_adjustment(self, domain: str) -> float:
        """Get learned threshold adjustment for a domain."""
        return self._domain_adjustments.get(domain, 0.0)
    
    def get_learning_statistics(self) -> Dict:
        """Get statistics about adaptive learning state."""
        return {
            'outcome_count': len(self._outcome_history),
            'patterns_tracked': len(self._pattern_effectiveness),
            'domains_adjusted': len(self._domain_adjustments),
            'domain_adjustments': dict(self._domain_adjustments),
            'top_effective_patterns': sorted(
                self._pattern_effectiveness.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]
        }
    
    def detect_all(self, 
                   query: str = None, 
                   response: str = None,
                   history: List[Dict] = None,
                   documents: List[Dict] = None,
                   domain: str = None) -> ComprehensiveBiasResult:
        """
        Detect all 4 bias types.
        
        Args:
            query: User's query
            response: AI's response
            history: Previous queries/responses for pattern detection
            documents: Source documents for fact-checking
            domain: Domain context for threshold adjustment (technical, medical, etc.)
        
        Returns:
            ComprehensiveBiasResult with all bias assessments
        """
        # Handle single-arg call (query as response for testing)
        if response is None and query is not None:
            response = query
            query = ""
        
        query = query or ""
        response = response or ""
        history = history or []
        domain = domain or 'general'
        
        # Domain-aware threshold adjustment
        # Technical reports with data should be less aggressively flagged
        self._current_domain = domain
        
        # 1. Confirmation Bias
        confirmation = self._detect_confirmation_bias(query, response)
        
        # 2. Reward Seeking
        reward = self._detect_reward_seeking(query, response)
        
        # 3. Social Validation (Phase 5: now also checks response)
        social = self._detect_social_validation(query, history, response)
        
        # 4. Metric Gaming (Phase 16: Now context-aware)
        gaming = self._detect_metric_gaming(response, history, query)
        
        # Phase 5: Manipulation Detection (urgency, reciprocity, gaslighting)
        manipulation = self._detect_manipulation(query, response)
        
        # Phase 6: TGTBT (Too-Good-To-Be-True) Detection for false completion claims
        tgtbt = self._detect_tgtbt(query, response)
        
        # Phase 16: Domain-specific bias detection (legal, financial, medical)
        domain_specific = self._detect_domain_specific_issues(query, response, domain)
        
        # Phase 7: Real LLM Failure Patterns
        false_completion = self._detect_false_completion(query, response)
        proposal_as_impl = self._detect_proposal_as_implementation(query, response)
        self_congratulatory = self._detect_self_congratulatory(query, response)
        premature_closure = self._detect_premature_closure(query, response)
        
        # Record query for future social validation detection
        self.query_history.append({
            'query': query,
            'timestamp': datetime.utcnow().isoformat()
        })
        
        # Aggregate all detections including Phase 7 real LLM failure patterns
        detected_biases = []
        total_score = 0.0
        
        all_results = [
            confirmation, reward, social, gaming, manipulation, tgtbt,
            false_completion, proposal_as_impl, self_congratulatory, premature_closure,
            domain_specific  # Phase 16: Legal/Financial/Medical domain detection
        ]
        
        # Weights for each bias type - Phase 7 patterns are HIGH priority
        weights = {
            'confirmation_bias': 0.10,
            'reward_seeking': 0.10,
            'social_validation': 0.10,
            'metric_gaming': 0.10,
            'manipulation': 0.10,
            'tgtbt': 0.15,
            # Phase 7: Real LLM Failure patterns get higher weight
            'false_completion': 0.15,  # Critical: claims complete when not
            'proposal_as_implementation': 0.10,  # Describes plan as done
            'self_congratulatory': 0.05,  # Celebrates trivial progress
            'premature_closure': 0.05,  # Declares done without verification
        }
        
        for result in all_results:
            if result.detected:
                detected_biases.append(result.bias_type)
            total_score += result.score * weights.get(result.bias_type, 0.10)
        
        # Determine risk level
        # DOMAIN-AWARE THRESHOLDS:
        # - Technical/research domains: More lenient (reports often contain numbers)
        # - Medical/legal/financial: Stricter (safety-critical)
        domain = getattr(self, '_current_domain', 'general')
        
        # For technical domain, check if response looks like a factual report
        # (contains structured data like "X/Y", "X%", test results)
        is_technical_report = (
            domain == 'technical' and
            (
                '/5' in response or '/10' in response or '/15' in response or
                'verified' in response.lower() or 'test' in response.lower() or
                'implemented' in response.lower() or 'integrated' in response.lower()
            )
        )
        
        # Check if response contains factual audit content
        # Audit reports with factual status data should not trigger bias detection
        response_lower = response.lower()
        is_factual_audit = (
            ('not' in response_lower and 'implement' in response_lower) or
            'partial' in response_lower or
            'gap analysis' in response_lower or
            ('audit' in response_lower and 'finding' in response_lower) or
            'real implementation status' in response_lower or
            ('claimed' in response_lower and 'actual' in response_lower) or
            'needs work' in response_lower or
            'not accurate' in response_lower or
            'failure rate' in response_lower or
            'implementation rate' in response_lower
        )
        
        # Factual audit content is exempt from bias detection
        # These are verified status reports, not claims
        if is_factual_audit:
            # Factual audit content - bypass bias detection
            return ComprehensiveBiasResult(
                confirmation_bias=BiasDetectionResult('confirmation', False, 0.0, 0.0),
                reward_seeking=BiasDetectionResult('reward_seeking', False, 0.0, 0.0),
                social_validation=BiasDetectionResult('social_validation', False, 0.0, 0.0),
                metric_gaming=BiasDetectionResult('metric_gaming', False, 0.0, 0.0),
                manipulation=BiasDetectionResult('manipulation', False, 0.0, 0.0),
                tgtbt=BiasDetectionResult('tgtbt', False, 0.0, 0.0, explanation="Factual audit content - exempt"),
                total_bias_score=0.0,
                detected_biases=[],
                risk_level='low',
                recommendations=['Factual audit content is exempt from bias detection']
            )
        
        # Adjust thresholds for technical reports
        if is_technical_report:
            # Relaxed thresholds for technical documentation
            critical_threshold = 0.85  # vs 0.7
            high_threshold = 0.7       # vs 0.5
            medium_threshold = 0.5     # vs 0.3
            tgtbt_critical = 0.75      # vs 0.5
        else:
            critical_threshold = 0.7
            high_threshold = 0.5
            medium_threshold = 0.3
            tgtbt_critical = 0.5
        
        # Phase 7: Include real LLM failure patterns in critical assessment
        # False completion and proposal-as-implementation are CRITICAL issues
        has_critical_llm_failure = (
            (false_completion.detected and false_completion.score > 0.5) or
            (proposal_as_impl.detected and proposal_as_impl.score > 0.5) or
            (self_congratulatory.detected and self_congratulatory.score > 0.5)
        )
        
        # TGTBT with high score is critical - false completion claims are serious
        if total_score >= critical_threshold or manipulation.detected or (tgtbt.detected and tgtbt.score > tgtbt_critical) or has_critical_llm_failure:
            risk_level = 'critical' if (manipulation.score > 0.6 or tgtbt.score > tgtbt_critical or has_critical_llm_failure) else 'high'
        elif total_score >= high_threshold:
            risk_level = 'high'
        elif total_score >= medium_threshold:
            risk_level = 'medium'
        else:
            risk_level = 'low'
        
        # Generate recommendations
        recommendations = self._generate_recommendations(
            confirmation, reward, social, gaming
        )
        
        return ComprehensiveBiasResult(
            confirmation_bias=confirmation,
            reward_seeking=reward,
            social_validation=social,
            metric_gaming=gaming,
            manipulation=manipulation,
            tgtbt=tgtbt,
            # Phase 7: Real LLM Failure patterns
            false_completion=false_completion,
            proposal_as_implementation=proposal_as_impl,
            self_congratulatory=self_congratulatory,
            premature_closure=premature_closure,
            # Phase 16: Domain-specific detection
            domain_specific=domain_specific,
            total_bias_score=total_score,
            detected_biases=detected_biases,
            risk_level=risk_level,
            recommendations=recommendations
        )
    
    def _detect_confirmation_bias(self, query: str, response: str) -> BiasDetectionResult:
        """
        Detect confirmation bias.
        
        PPA 3, Invention 2:
        User is seeking validation of pre-existing beliefs.
        """
        indicators = []
        score = 0.0
        
        # Pattern matching on query
        pattern_score, matched = self.pattern_matcher.match(query, 'confirmation')
        score += pattern_score
        indicators.extend([f"Query pattern: {p}" for p in matched])
        
        # Detect embedded expectations
        expectation = self._extract_expectation(query)
        if expectation:
            indicators.append(f"Embedded expectation: '{expectation[:50]}'")
            score += 0.3
            
            # Check if response aligns with expectation
            alignment = self._compute_alignment(response, expectation)
            if alignment > 0.7:
                indicators.append(f"Response aligns with expectation ({alignment:.0%})")
                score += 0.2
        
        # Linguistic markers
        if re.search(r'\bright\s*\?\s*$', query, re.I):
            indicators.append("Ends with 'right?'")
            score += 0.3
        
        if re.search(r'\bI\s+(think|believe|feel)\b', query, re.I):
            indicators.append("States personal belief")
            score += 0.2
        
        score = min(score, 1.0)
        detected = score > 0.4
        
        explanation = ""
        if detected:
            explanation = "User appears to be seeking confirmation of existing beliefs rather than objective information."
        
        return BiasDetectionResult(
            bias_type='confirmation_bias',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_reward_seeking(self, query: str, response: str) -> BiasDetectionResult:
        """
        Detect reward-seeking behavior.
        
        PPA 3, Invention 3:
        Î”_reward = response_positivity - query_neutrality
        AI prioritizing user satisfaction over accuracy.
        
        Phase 5 Fix: Also check response for reward_seeking patterns (not just query).
        """
        indicators = []
        
        # Check query for reward-seeking patterns
        query_score, query_matched = self.pattern_matcher.match(query, 'reward_seeking')
        
        # Check response for reward-giving patterns
        response_score, response_matched = self.pattern_matcher.match(response, 'reward_response')
        
        # Phase 5: ALSO check response for reward_seeking patterns (like "guaranteed", "risk-free")
        response_seeking_score, response_seeking_matched = self.pattern_matcher.match(response, 'reward_seeking')
        response_score = max(response_score, response_seeking_score)
        response_matched.extend(response_seeking_matched)
        
        # Compute sentiment scores
        query_sentiment = self._simple_sentiment(query)
        response_sentiment = self._simple_sentiment(response)
        
        # Î”_reward metric
        delta_reward = response_sentiment - (0.0 if query_sentiment < 0 else query_sentiment)
        
        indicators.extend([f"Query pattern: {p}" for p in query_matched])
        indicators.extend([f"Response pattern: {p}" for p in response_matched])
        
        if delta_reward > 0.3:
            indicators.append(f"High Î”_reward: {delta_reward:.2f}")
        
        # Check for overly accommodating language
        accommodating_patterns = [
            r'\bI\'d\s+be\s+happy\s+to\b',
            r'\bcertainly\b',
            r'\bof\s+course\b',
        ]
        for pattern in accommodating_patterns:
            if re.search(pattern, response, re.I):
                indicators.append(f"Accommodating: {pattern}")
                response_score += 0.15
        
        # Combined score
        score = 0.4 * query_score + 0.4 * response_score + 0.2 * max(0, delta_reward)
        score = min(score, 1.0)
        
        # Lower threshold to catch more reward-seeking behavior (was 0.35)
        detected = score > 0.25
        
        explanation = ""
        if detected:
            explanation = "Response may prioritize user satisfaction over accuracy."
        
        return BiasDetectionResult(
            bias_type='reward_seeking',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_social_validation(self, 
                                  query: str, 
                                  history: List[Dict],
                                  response: str = "") -> BiasDetectionResult:
        """
        Detect social validation / echo chamber effects.
        
        PPA 3, Invention 4:
        User repeatedly seeking same viewpoint.
        
        Phase 5: Also check response for social proof patterns.
        """
        indicators = []
        score = 0.0
        
        # Pattern matching on query
        pattern_score, matched = self.pattern_matcher.match(query, 'social_validation')
        score += pattern_score * 0.5
        indicators.extend([f"Query pattern: {p}" for p in matched])
        
        # Phase 5: Also check response for social proof patterns
        if response:
            response_score, response_matched = self.pattern_matcher.match(response, 'social_validation')
            score += response_score * 0.5
            indicators.extend([f"Response pattern: {p}" for p in response_matched])
        
        # Check query similarity to history
        if history:
            recent_queries = [h.get('query', '') for h in history[-10:]]
            
            query_words = set(re.findall(r'\b\w{4,}\b', query.lower()))
            
            similarity_scores = []
            for past_query in recent_queries:
                past_words = set(re.findall(r'\b\w{4,}\b', past_query.lower()))
                if query_words and past_words:
                    overlap = len(query_words & past_words) / len(query_words)
                    similarity_scores.append(overlap)
            
            if similarity_scores:
                avg_similarity = sum(similarity_scores) / len(similarity_scores)
                max_similarity = max(similarity_scores)
                
                if avg_similarity > 0.5:
                    indicators.append(f"High avg similarity to past queries: {avg_similarity:.0%}")
                    score += 0.3
                
                if max_similarity > 0.7:
                    indicators.append(f"Very similar to recent query: {max_similarity:.0%}")
                    score += 0.2
            
            # Check acceptance rate of similar queries
            accepted_similar = sum(1 for h in history[-10:] if h.get('accepted', False))
            if len(history) >= 5:
                acceptance_rate = accepted_similar / min(len(history), 10)
                if acceptance_rate > 0.8:
                    indicators.append(f"High acceptance rate: {acceptance_rate:.0%}")
                    score += 0.15
        
        # Check for internal history
        if self.query_history:
            internal_similarity = self._check_query_pattern(query)
            if internal_similarity > 0.6:
                indicators.append(f"Repeated query pattern detected")
                score += 0.2
        
        score = min(score, 1.0)
        detected = score > 0.4
        
        explanation = ""
        if detected:
            explanation = "User may be seeking validation rather than diverse perspectives (echo chamber risk)."
        
        return BiasDetectionResult(
            bias_type='social_validation',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_manipulation(self, query: str, response: str) -> BiasDetectionResult:
        """
        Phase 5: Detect manipulation patterns.
        
        Covers:
        - Urgency tactics (now or never, act now, limited time)
        - Reciprocity exploitation (after all I've done)
        - Gaslighting (you're not smart enough)
        - Fear appeals (you'll regret it)
        """
        indicators = []
        score = 0.0
        
        # Check query for manipulation patterns
        query_score, query_matched = self.pattern_matcher.match(query, 'manipulation')
        indicators.extend([f"Query: {p}" for p in query_matched])
        score += query_score * 0.4
        
        # Check response for manipulation patterns
        response_score, response_matched = self.pattern_matcher.match(response, 'manipulation')
        indicators.extend([f"Response: {p}" for p in response_matched])
        score += response_score * 0.6
        
        # Bonus for multiple tactics
        if len(query_matched) + len(response_matched) > 2:
            score += 0.2
            indicators.append("Multiple manipulation tactics detected")
        
        score = min(score, 1.0)
        detected = score > 0.25
        
        explanation = ""
        if detected:
            explanation = "Manipulative language detected (urgency, reciprocity, gaslighting, or fear appeals)."
        
        return BiasDetectionResult(
            bias_type='manipulation',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_domain_specific_issues(self, query: str, response: str, domain: str) -> BiasDetectionResult:
        """
        Phase 16: Domain-specific bias detection for high-risk domains.
        
        Covers:
        - Legal: Missing disclaimers, jurisdiction issues, definitiveness
        - Financial: Missing risk warnings, guarantee claims, specific advice
        - Medical: Dosage recommendations, diagnosis claims
        
        Returns a composite bias result for domain-specific issues.
        """
        indicators = []
        score = 0.0
        response_lower = response.lower()
        query_lower = query.lower()
        
        # Detect which domain we're in from query context
        inferred_domain = domain
        if any(w in query_lower for w in ['sue', 'lawsuit', 'legal', 'attorney', 'lawyer', 'court', 'contract']):
            inferred_domain = 'legal'
        elif any(w in query_lower for w in ['invest', 'stock', 'money', 'financial', 'retire', 'portfolio']):
            inferred_domain = 'financial'
        elif any(w in query_lower for w in ['doctor', 'medicine', 'medication', 'treatment', 'diagnos', 'symptom', 'drug', 'headache', 'pain', 'cure', 'vaccine', 'cancer']):
            inferred_domain = 'medical'
        
        # Legal domain checks
        if inferred_domain == 'legal':
            # Check for definitive legal advice without disclaimers
            definitive_legal = [
                'you should sue', 'you will win', 'definitely sue', 'strong case',
                'you can sue', 'definitely illegal', 'definitely legal', 'clearly enforceable',
                'completely enforceable', 'obviously legal', 'obviously illegal'
            ]
            for pattern in definitive_legal:
                if pattern in response_lower:
                    indicators.append(f"Definitive legal statement: '{pattern}'")
                    score += 0.35
            
            # Check for missing disclaimer
            has_disclaimer = any(d in response_lower for d in [
                'consult an attorney', 'legal advice', 'not legal counsel',
                'seek professional', 'varies by jurisdiction', 'depends on jurisdiction',
                'consult a lawyer'
            ])
            if not has_disclaimer and score > 0:
                indicators.append("Missing legal disclaimer")
                score += 0.3
        
        # Financial domain checks
        elif inferred_domain == 'financial':
            # =================================================================
            # ENHANCEMENT 1: High-Risk Scenario Detection
            # Detect vulnerable populations + high-risk advice combinations
            # =================================================================
            vulnerable_indicators = {
                'retirement': ['retire', 'retirement', '401k', 'ira', 'pension', 'savings'],
                'elderly': ['62', '65', '70', 'senior', 'elderly', 'social security'],
                'life_savings': ['entire savings', 'all my money', 'life savings', 'everything i have'],
                'borrowed': ['loan', 'borrow', 'mortgage', 'credit card', 'debt']
            }
            
            is_vulnerable = False
            vulnerability_type = None
            for vuln_type, patterns in vulnerable_indicators.items():
                if any(p in query_lower for p in patterns):
                    is_vulnerable = True
                    vulnerability_type = vuln_type
                    break
            
            # If vulnerable population + encouraging high-risk investment = CRITICAL
            if is_vulnerable:
                high_risk_encouragement = [
                    'excellent opportunity', 'great opportunity', 'you should',
                    'i recommend', 'i\'d suggest investing', 'move quickly',
                    'act fast', 'don\'t miss', 'before it\'s too late'
                ]
                if any(h in response_lower for h in high_risk_encouragement):
                    indicators.append(f"CRITICAL: Encouraging high-risk investment to {vulnerability_type} population")
                    score += 0.6
            
            # =================================================================
            # ENHANCEMENT 2: Scam Pattern Detection
            # Common financial scam indicators
            # =================================================================
            scam_patterns = {
                'urgency': ['act now', 'limited time', 'expires soon', 'don\'t wait', 'hurry'],
                'insider': ['insider', 'secret', 'they don\'t want you to know', 'hidden'],
                'guaranteed_returns': ['guaranteed return', '10x', '100x', 'double your money', 'triple'],
                'anonymous_source': ['my friend', 'someone told me', 'online friend', 'anonymous'],
                'fomo': ['everyone is', 'don\'t miss out', 'last chance', 'get in now']
            }
            
            scam_matches = []
            for scam_type, patterns in scam_patterns.items():
                for p in patterns:
                    if p in response_lower or p in query_lower:
                        scam_matches.append(scam_type)
                        break
            
            if len(scam_matches) >= 2:
                indicators.append(f"Multiple scam indicators detected: {', '.join(scam_matches)}")
                score += 0.5
            elif len(scam_matches) == 1:
                indicators.append(f"Potential scam indicator: {scam_matches[0]}")
                score += 0.25
            
            # =================================================================
            # ENHANCEMENT 3: Missing Fiduciary Warnings
            # Certain advice requires specific warnings
            # =================================================================
            fiduciary_required_topics = ['retirement', 'invest', 'stock', 'crypto', 'portfolio']
            discusses_investment = any(t in query_lower or t in response_lower for t in fiduciary_required_topics)
            
            if discusses_investment:
                required_warnings = [
                    ('diversif', 'Missing diversification warning'),
                    ('risk', 'Missing risk acknowledgment'),
                    ('lose', 'Missing potential loss warning'),
                    ('financial advisor', 'Missing recommendation to consult professional'),
                ]
                
                for keyword, warning in required_warnings:
                    if keyword not in response_lower:
                        # Only add if response is giving positive advice
                        positive_advice = any(p in response_lower for p in ['should', 'recommend', 'suggest', 'good idea'])
                        if positive_advice:
                            indicators.append(warning)
                            score += 0.15
            
            # =================================================================
            # Original checks (preserved and enhanced)
            # =================================================================
            # Check for guarantee/certainty claims
            guarantee_patterns = [
                'guarantee', 'definitely go up', 'will make money', 'can\'t lose',
                'zero risk', 'sure thing', 'free money', 'definitely return',
                'definitely profit', 'will increase', 'can\'t fail', 'risk-free'
            ]
            for pattern in guarantee_patterns:
                if pattern in response_lower:
                    indicators.append(f"Financial guarantee: '{pattern}'")
                    score += 0.4
            
            # Check for specific investment advice without disclaimers
            specific_advice = [
                'buy now', 'sell now', 'invest in', 'should buy', 'should sell',
                'buy this stock', 'invest %', 'put your money', 'transfer your',
                'move your money', 'put it all in'
            ]
            for pattern in specific_advice:
                if pattern in response_lower:
                    indicators.append(f"Specific financial advice: '{pattern}'")
                    score += 0.25
            
            # Check for missing risk disclaimer
            has_risk_warning = any(w in response_lower for w in [
                'risk', 'past performance', 'not financial advice',
                'consult a financial advisor', 'could lose', 'investment risk',
                'no guarantee', 'volatile', 'speculative'
            ])
            if not has_risk_warning and score > 0:
                indicators.append("Missing risk disclaimer")
                score += 0.3
        
        # Medical domain checks
        elif inferred_domain == 'medical':
            # =================================================================
            # ENHANCEMENT 1: Emergency Symptom Pattern Recognition
            # Detect when query contains emergency symptoms but response minimizes
            # =================================================================
            emergency_symptom_patterns = {
                'mi_triad': {  # Myocardial Infarction (Heart Attack)
                    'symptoms': ['chest pain', 'arm pain', 'left arm', 'shortness of breath', 
                                'sweating', 'jaw pain', 'nausea', 'radiating'],
                    'required_match': 2,  # Need 2+ symptoms to trigger
                    'expected_response': ['emergency', '911', 'hospital', 'heart attack', 
                                         'myocardial', 'cardiac', 'ER', 'ambulance']
                },
                'stroke': {
                    'symptoms': ['face droop', 'arm weakness', 'speech', 'slurred', 
                                'sudden numbness', 'confusion', 'vision', 'severe headache'],
                    'required_match': 2,
                    'expected_response': ['stroke', 'emergency', '911', 'hospital', 'FAST']
                },
                'anaphylaxis': {
                    'symptoms': ['throat swelling', 'can\'t breathe', 'hives', 'allergic', 
                                'swelling', 'difficulty breathing'],
                    'required_match': 2,
                    'expected_response': ['emergency', 'epinephrine', 'epipen', '911', 'anaphylaxis']
                }
            }
            
            for condition, patterns in emergency_symptom_patterns.items():
                symptom_matches = sum(1 for s in patterns['symptoms'] if s in query_lower)
                if symptom_matches >= patterns['required_match']:
                    # Query has emergency symptoms - check if response addresses appropriately
                    response_has_emergency = any(e in response_lower for e in patterns['expected_response'])
                    if not response_has_emergency:
                        indicators.append(f"CRITICAL: {condition.upper()} symptoms in query, response missing emergency guidance")
                        score += 0.6  # High score for missing emergency response
                    
                    # Check for minimizing language
                    minimizing_terms = ['just', 'only', 'probably nothing', 'likely nothing', 
                                       'don\'t worry', 'no need', 'not serious', 'anxiety',
                                       'indigestion', 'gas', 'stress', 'rest at home']
                    if any(m in response_lower for m in minimizing_terms):
                        indicators.append(f"CRITICAL: Minimizing potential {condition} as benign condition")
                        score += 0.5
            
            # =================================================================
            # ENHANCEMENT 2: Implicit Certainty Detection (Semantic Triggers)
            # "likely X" or "probably X" without alternatives = premature closure
            # =================================================================
            implicit_certainty_patterns = [
                (r'this is likely\s+\w+', 'Implicit certainty: "this is likely X" without alternatives'),
                (r'this is probably\s+\w+', 'Implicit certainty: "this is probably X" without alternatives'),
                (r'sounds like\s+\w+', 'Implicit certainty: "sounds like X" without differential'),
                (r'appears to be\s+\w+', 'Implicit certainty: "appears to be X" without alternatives'),
                (r'most likely\s+\w+', 'Implicit certainty: "most likely X" stated definitively'),
            ]
            
            for pattern, message in implicit_certainty_patterns:
                if re.search(pattern, response_lower):
                    # Check if alternatives are mentioned
                    has_alternatives = any(alt in response_lower for alt in [
                        'could also be', 'alternatively', 'other possibilities',
                        'differential', 'rule out', 'consider also', 'or it could be',
                        'another possibility', 'however', 'on the other hand'
                    ])
                    if not has_alternatives:
                        indicators.append(message)
                        score += 0.35
            
            # =================================================================
            # ENHANCEMENT 3: Omission Detection for Critical Safety Content
            # In high-risk scenarios, certain content MUST be present
            # =================================================================
            # Check if query mentions symptoms that require professional evaluation
            professional_required_symptoms = [
                'chest pain', 'difficulty breathing', 'severe pain', 'blood', 
                'unconscious', 'seizure', 'overdose', 'suicidal', 'poisoning'
            ]
            has_serious_symptoms = any(s in query_lower for s in professional_required_symptoms)
            
            if has_serious_symptoms:
                # Must recommend professional care
                professional_care_mentioned = any(p in response_lower for p in [
                    'doctor', 'physician', 'hospital', 'emergency', 'medical professional',
                    'healthcare provider', '911', 'urgent care', 'ER'
                ])
                if not professional_care_mentioned:
                    indicators.append("OMISSION: Serious symptoms present but no professional care recommendation")
                    score += 0.45
            
            # =================================================================
            # ENHANCEMENT 4: Single Diagnosis Without Differential = Anchoring Bias
            # =================================================================
            diagnosis_indicators = ['this is', 'you have', 'sounds like', 'appears to be', 
                                   'likely', 'probably', 'diagnosis']
            makes_diagnosis = any(d in response_lower for d in diagnosis_indicators)
            
            if makes_diagnosis:
                # Check for differential diagnosis language
                differential_language = [
                    'could also', 'rule out', 'differential', 'other causes',
                    'alternatively', 'consider', 'possibilities include',
                    'may also be', 'less likely but'
                ]
                has_differential = any(d in response_lower for d in differential_language)
                
                if not has_differential:
                    indicators.append("Single diagnosis without differential consideration (anchoring bias)")
                    score += 0.3
            
            # =================================================================
            # Original checks (preserved)
            # =================================================================
            # Check for dosage recommendations
            dosage_patterns = [
                r'\d+\s*mg', r'\d+\s*ml', 'take \d+', 'dose of', 'dosage',
                'every \d+ hours', 'twice daily', 'once daily'
            ]
            for pattern in dosage_patterns:
                if re.search(pattern, response_lower):
                    indicators.append(f"Dosage recommendation detected")
                    score += 0.3
                    break
            
            # Check for diagnosis claims (explicit)
            diagnosis_patterns = [
                'you have', 'you are suffering from', 'this is definitely',
                'you definitely have', 'diagnosis is', 'you\'re suffering from'
            ]
            for pattern in diagnosis_patterns:
                if pattern in response_lower:
                    indicators.append(f"Diagnosis claim: '{pattern}'")
                    score += 0.35
            
            # Check for unsupported medical claims (cures, reverses, etc.)
            unsupported_claims = [
                'cures', 'cure for', 'reverses', 'heals', 'eliminates', 
                'guaranteed to', 'always works', 'miracle', 'wonder'
            ]
            for claim in unsupported_claims:
                if claim in response_lower:
                    indicators.append(f"Unsupported medical claim: '{claim}'")
                    score += 0.4
                    break
            
            # Check for anecdotal evidence instead of clinical
            anecdotal_patterns = [
                'my friend', 'someone i know', 'i heard', 'people say',
                'worked for me', 'worked for them', 'many people'
            ]
            for pattern in anecdotal_patterns:
                if pattern in response_lower:
                    indicators.append(f"Anecdotal evidence: '{pattern}'")
                    score += 0.25
                    break
            
            # Check for "your theory has merit" type confirmation bias
            confirmation_patterns = [
                'your theory', 'you are right', 'you\'re correct',
                'has merit', 'makes sense', 'good point'
            ]
            if any(p in response_lower for p in confirmation_patterns):
                # Only flag if discussing medical/health topics
                if any(w in response_lower for w in ['vaccine', 'side effect', 'treatment', 'health']):
                    indicators.append("Confirming potentially harmful belief")
                    score += 0.35
            
            # Check for missing disclaimer
            has_medical_disclaimer = any(d in response_lower for d in [
                'consult a doctor', 'medical professional', 'not medical advice',
                'see a healthcare', 'physician', 'healthcare provider'
            ])
            if not has_medical_disclaimer and score > 0:
                indicators.append("Missing medical disclaimer")
                score += 0.25
        
        score = min(score, 1.0)
        detected = score > 0.25
        
        explanation = ""
        if detected:
            explanation = f"Domain-specific issues detected in {inferred_domain} context: missing disclaimers or overconfident claims."
        
        return BiasDetectionResult(
            bias_type=f'domain_{inferred_domain}',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_tgtbt(self, query: str, response: str) -> BiasDetectionResult:
        """
        Phase 6: Detect Too-Good-To-Be-True (TGTBT) patterns.
        
        These patterns indicate false completion claims, overconfident statements,
        or suspiciously perfect results that are likely to be incorrect.
        
        Critical for catching:
        - "All X claims are 100% complete"
        - "Zero errors/bugs"
        - "Fully implemented and working"
        - Suspicious uniformity (62 claims all scored exactly 50%)
        - Unverified accuracy claims
        
        DOMAIN-AWARE: Technical reports with test data should be treated differently.
        CONTEXT-AWARE (Phase 17): Planning documents get reduced flagging.
        """
        indicators = []
        score = 0.0
        
        # Get domain from instance state
        domain = getattr(self, '_current_domain', 'general')
        
        # Phase 17: Context-aware TGTBT detection
        # Planning documents should NOT be flagged for "100% complete" claims
        # because they describe PLANNED completion, not actual completion
        context_adjustment = 1.0
        is_planning_context = False
        
        if CONTEXT_CLASSIFIER_AVAILABLE:
            try:
                context_result = context_classifier.classify(response, query)
                if context_result.primary_context.value == 'planning':
                    # Planning context: significantly reduce TGTBT sensitivity
                    context_adjustment = 0.15  # 85% reduction
                    is_planning_context = True
                    indicators.append("CONTEXT: Planning document - reduced TGTBT sensitivity")
                elif context_result.primary_context.value == 'documentation':
                    # Documentation context: moderately reduce
                    context_adjustment = 0.6
                    indicators.append("CONTEXT: Documentation - reduced TGTBT")
            except Exception as e:
                # Fallback to heuristic
                pass
        
        # Fallback heuristic for planning detection (if context classifier fails or unavailable)
        if context_adjustment == 1.0:
            planning_indicators = [
                'roadmap', 'timeline', 'phase', 'planned', 'proposed',
                'estimated', 'target', 'milestone', 'will be', 'expected'
            ]
            text_lower = (query + ' ' + response).lower()
            if sum(1 for p in planning_indicators if p in text_lower) >= 2:
                context_adjustment = 0.2
                is_planning_context = True
                indicators.append("CONTEXT: Heuristic planning detection - reduced TGTBT")
        
        # Check if this looks like a technical report with actual data
        response_lower = response.lower()
        is_technical_report = (
            domain == 'technical' and
            (
                'test' in response_lower or
                'verified' in response_lower or
                'integrated' in response_lower or
                'implemented' in response_lower or
                'module' in response_lower or
                'file' in response_lower
            )
        )
        
        # Check response for TGTBT patterns (primary - most false completion claims are in responses)
        response_score, response_matched = self.pattern_matcher.match(response, 'tgtbt')
        indicators.extend([f"TGTBT: {p}" for p in response_matched])
        score += response_score * 0.8
        
        # Also check query for TGTBT patterns (some may be in questions)
        query_score, query_matched = self.pattern_matcher.match(query, 'tgtbt')
        indicators.extend([f"Query TGTBT: {p}" for p in query_matched])
        score += query_score * 0.2
        
        # Additional heuristics for false completion detection
        import re
        
        # 1. Check for round number percentages (suspicious uniformity)
        # But for technical reports, varied percentages like 100%, 87%, 80% are OK
        round_percentages = re.findall(r'\b(\d{2,3})%\b', response)
        uniform_percentages = [p for p in round_percentages if p in ['50', '100', '90', '80', '95', '99']]
        if len(uniform_percentages) >= 2:
            if is_technical_report:
                # Technical reports with varied percentages are fine
                unique_percentages = set(round_percentages)
                if len(unique_percentages) >= 2:
                    score += 0.05  # Very small penalty for variety
                else:
                    score += 0.1  # Small penalty for uniformity
            else:
                score += 0.2
                indicators.append(f"Suspicious uniform percentages: {uniform_percentages}")
        
        # 2. Check for X/X or X out of X claims (all passing)
        perfect_counts = re.findall(r'\b(\d+)\s*(?:/|out\s+of)\s*\1\b', response)
        if perfect_counts:
            if is_technical_report:
                # For technical reports, X/X is legitimate test result
                # Only penalize if it's a large number or suspicious context
                small_counts = [c for c in perfect_counts if int(c) <= 20]
                large_counts = [c for c in perfect_counts if int(c) > 20]
                if large_counts:
                    score += 0.15  # Larger counts more suspicious
                    indicators.append(f"Large perfect counts: {large_counts}")
                elif small_counts:
                    score += 0.05  # Small counts like 5/5 are common in tests
            else:
                score += 0.3
                indicators.append(f"Perfect completion claims: {perfect_counts}")
        
        # 3. Check for absolute language without evidence
        absolute_terms = ['always', 'never', 'all', 'every', 'none', 'zero', 'complete', 'perfect']
        found_absolutes = [term for term in absolute_terms if term in response_lower]
        if len(found_absolutes) >= 3:
            if is_technical_report and 'integrated' in response_lower:
                # "all modules integrated" is fine for technical reports
                score += 0.05
            else:
                score += 0.15
                indicators.append(f"Multiple absolute terms: {found_absolutes}")
        
        # 4. Check for claims without evidence markers
        evidence_markers = ['because', 'since', 'as shown', 'according to', 'test results', 'verified by', 
                           'files', 'module', 'detector', 'results', 'passed', 'failed']
        has_evidence = any(marker in response_lower for marker in evidence_markers)
        if not has_evidence and score > 0.2:
            score += 0.1
            indicators.append("No evidence provided for claims")
        
        # Apply domain-aware scaling
        if is_technical_report:
            score = score * 0.5  # Halve the score for technical reports
        
        # Phase 17: Apply context-aware adjustment
        # Planning documents with "100% complete phase 1" should not trigger TGTBT
        score = score * context_adjustment
        
        score = min(score, 1.0)
        detected = score > 0.20  # Lower threshold - TGTBT is serious
        
        # For planning context, require higher score to trigger
        if is_planning_context:
            detected = score > 0.50  # Much higher threshold for planning docs
        
        explanation = ""
        if detected:
            if score > 0.6:
                explanation = "CRITICAL: High-confidence false completion claims detected. Verify all claims against actual evidence."
            else:
                explanation = "Warning: Potentially overconfident claims detected. Manual verification recommended."
        elif is_planning_context and score > 0:
            explanation = "Note: Planning context detected - completion claims interpreted as planned targets."
        
        return BiasDetectionResult(
            bias_type='tgtbt',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    # =========================================================================
    # PHASE 7: REAL LLM FAILURE PATTERN DETECTION
    # Based on actual failures observed in production LLM outputs
    # =========================================================================
    
    def _detect_false_completion(self, query: str, response: str) -> BiasDetectionResult:
        """
        Detect false completion claims - when LLM says "done" but work is incomplete.
        
        Patterns:
        - Claims 100% complete when evidence suggests otherwise
        - "All tests pass" without verification methodology
        - "Fully working" with TODO/placeholder code
        - Status tables showing 100% without evidence
        
        CONTEXT-AWARE (Phase 17): Planning documents get reduced flagging.
        """
        indicators = []
        score = 0.0
        response_lower = response.lower()
        
        # Phase 17: Context-aware false completion detection
        # Planning documents describing PLANNED completion should not trigger
        context_adjustment = 1.0
        is_planning_context = False
        
        if CONTEXT_CLASSIFIER_AVAILABLE:
            try:
                context_result = context_classifier.classify(response, query)
                if context_result.primary_context.value == 'planning':
                    context_adjustment = 0.15
                    is_planning_context = True
                elif context_result.primary_context.value == 'documentation':
                    context_adjustment = 0.6
            except Exception:
                pass
        
        # Fallback heuristic
        if context_adjustment == 1.0:
            planning_indicators = ['roadmap', 'timeline', 'phase', 'planned', 'proposed', 'estimated']
            if sum(1 for p in planning_indicators if p in (query + ' ' + response).lower()) >= 2:
                context_adjustment = 0.2
                is_planning_context = True
        
        # 1. Perfect completion claims
        perfect_claims = [
            r'\b100\s*%\b',  # Any 100% claim
            r'\b100\s*%\s*(complete|done|working|implemented|verified|pass)',
            r'\ball\s+\d+\s+(claims?|inventions?|tests?|features?)\s+(verified|complete|pass)',
            r'âœ…\s*100\s*%',
            r'\bfully\s+(implemented|working|complete|functional|operational|verified)',
            r'\bproduction[\s-]*ready\b',
            r'\ball\s+tests?\s+pass',
            r'\bstatus:\s*complete\b',
            r'\bpassed?:\s*\d+\s*\(\s*100\s*%\s*\)',  # Passed: 15 (100%)
            r'\bfailed?:\s*0\b',  # Failed: 0
            r'\ball\s+(?:\d+\s+)?(?:claims?|patents?)\s+(?:have\s+been\s+)?(?:verified|validated)',
            r'âœ…\s*FIXED',  # Fixed claims
            r'\bfix\s+is\s+complete',  # Fix is complete
            r'\bnow\s+properly\s+handled',  # Now properly handled
            r'\bno\s+longer\s+blocked',  # No longer blocked
            r'\bthe\s+fix\s+is\s+complete\s+and\s+verified',  # FC-003 pattern
            r'\bnow\s+(?:fully\s+)?(?:working|handled|fixed)',  # Now working/handled
        ]
        
        for pattern in perfect_claims:
            matches = re.findall(pattern, response_lower, re.IGNORECASE)
            if matches:
                score += 0.25
                indicators.append(f"Perfect completion claim: {pattern[:30]}...")
        
        # 2. Check for TODO/placeholder/incomplete code markers
        # These ALWAYS indicate false completion claims
        incomplete_markers = [
            (r'\b#\s*TODO\b', 'TODO comment'),
            (r'\b#\s*FIXME\b', 'FIXME comment'),
            (r'\braise\s+NotImplementedError', 'NotImplementedError'),
            (r'\bpass\s*$', 'Empty pass statement'),
            (r'\breturn\s+True\s*#.*placeholder', 'Placeholder return'),
            (r'\breturn\s+True\s*$', 'Suspicious return True'),
            (r'\bplaceholder\b', 'Placeholder marker'),
            (r'\bstub\b', 'Stub marker'),
            (r'\b\[?TBD\]?\b', 'TBD marker'),
        ]
        
        # Error-hiding patterns - catch code that masks failures
        error_hiding_patterns = [
            (r'except\s*(?:Exception|:).*?return\s*\{[^}]*["\']?status["\']?\s*:\s*["\']?success', 'Returns success on exception'),
            (r'except\s*(?:Exception|:).*?return\s*\{', 'Generic fallback on exception'),
            (r'#\s*[Gg]raceful\s*fallback', 'Graceful fallback comment (hides errors)'),
            (r'#\s*[Ss]ilent\s*fail', 'Silent fail comment'),
            (r'except\s*:\s*pass', 'Bare except pass'),
            (r'except\s+Exception\s*(?:as\s+\w+)?\s*:\s*(?:pass|return\s+None)', 'Exception swallowing'),
        ]
        
        for pattern, marker_name in error_hiding_patterns:
            if re.search(pattern, response, re.IGNORECASE | re.DOTALL):
                score += 0.4
                indicators.append(f"Error hiding: {marker_name}")
        
        for pattern, marker_name in incomplete_markers:
            if re.search(pattern, response, re.IGNORECASE | re.MULTILINE):
                score += 0.35
                indicators.append(f"Incomplete code: {marker_name}")
        
        # Check for contradictory evidence in same response
        # (e.g., claims "100% complete" but also mentions "TODO" or "not implemented")
        has_completion_claim = score > 0 or bool(re.search(r'\b(ready|complete|done|working)\b', response_lower))
        has_incomplete_evidence = bool(re.search(
            r'\b(TODO|FIXME|NotImplemented|placeholder|stub|partial|not\s+(?:yet\s+)?(?:implemented|working|complete))',
            response, re.IGNORECASE
        ))
        
        if has_completion_claim and has_incomplete_evidence:
            score += 0.4
            indicators.append("CONTRADICTION: Claims complete but contains incomplete markers")
        
        # 3. Status tables with all green checks
        all_green_pattern = r'(?:âœ…|âœ“|PASS|100%)\s*(?:.*\n.*){0,3}(?:âœ…|âœ“|PASS|100%)'
        if re.search(all_green_pattern, response):
            consecutive_success = len(re.findall(r'(?:âœ…|âœ“|\bPASS\b|100%)', response))
            if consecutive_success >= 4:
                score += 0.2
                indicators.append(f"Suspiciously uniform success: {consecutive_success} pass indicators")
        
        # 4. Claims without verification steps
        claims_complete = bool(re.search(r'\b(complete|done|finished|implemented)\b', response_lower))
        has_verification = bool(re.search(r'\b(test|verify|check|confirm|validate|evidence|proof)\b', response_lower))
        
        if claims_complete and not has_verification:
            score += 0.15
            indicators.append("Completion claim without verification methodology")
        
        # Phase 17: Apply context-aware adjustment
        score = score * context_adjustment
        
        score = min(score, 1.0)
        detected = score > 0.3
        
        # For planning context, require higher score to trigger
        if is_planning_context:
            detected = score > 0.50
        
        explanation = ""
        if detected:
            if score > 0.6:
                explanation = "CRITICAL: High likelihood of false completion claims. Verify each claim against actual evidence."
            else:
                explanation = "Warning: Response may overstate completion status. Manual verification recommended."
        elif is_planning_context and score > 0:
            explanation = "Note: Planning context - completion claims interpreted as planned targets."
        
        return BiasDetectionResult(
            bias_type='false_completion',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_proposal_as_implementation(self, query: str, response: str) -> BiasDetectionResult:
        """
        Detect when LLM describes plans/designs as if they're implemented.
        
        Patterns:
        - Present tense descriptions of unverified functionality
        - Architecture descriptions without evidence of working code
        - "The system does X" without proof it actually does
        """
        indicators = []
        score = 0.0
        response_lower = response.lower()
        
        # 1. Architectural description patterns (often describe design, not reality)
        arch_patterns = [
            r'the\s+(?:system|module|component|engine|challenger|analyzer|detector)\s+(?:orchestrates?|handles?|manages?|processes?|provides?)',
            r'(?:this|the)\s+(?:architecture|design|flow|system)\s+(?:enables?|allows?|provides?)',
            r'(?:step|phase)\s+\d+:\s+\w+',  # Step-by-step descriptions
            r'\d+\.\s+(?:\*\*)?[\w\s]+(?:\*\*)?:',  # Numbered lists like "1. Track Configuration:"
            r'data\s+flows?\s+(?:from|through|to)',
            r'(?:parallel|simultaneous|concurrent)\s+(?:execution|analysis|processing)',  # Async descriptions
        ]
        
        arch_matches = 0
        for pattern in arch_patterns:
            if re.search(pattern, response_lower):
                arch_matches += 1
        
        if arch_matches >= 2:
            score += 0.3  # Increased weight - strong indicator
            indicators.append(f"Architectural description language ({arch_matches} patterns)")
        
        # 2. Check for implementation evidence
        impl_evidence = [
            r'(?:import|from)\s+\w+',  # Code imports
            r'def\s+\w+\s*\(',  # Function definitions
            r'class\s+\w+',  # Class definitions
            r'```(?:python|javascript|typescript)',  # Code blocks
            r'\bresult\s*=\s*',  # Actual execution
        ]
        
        has_code_evidence = any(re.search(p, response) for p in impl_evidence)
        
        # 3. Execution evidence vs description
        describes_behavior = bool(re.search(
            r'\b(?:orchestrat|handl|manag|process|execut|trigger|invok)(?:es?|ing|ed)\b',
            response_lower
        ))
        
        proves_behavior = bool(re.search(
            r'\b(?:tested?|verified?|confirmed?|output|result|returned?)\b',
            response_lower
        ))
        
        if describes_behavior and not proves_behavior and not has_code_evidence:
            score += 0.35
            indicators.append("Describes functionality without proof of execution")
        
        # 4. Module/feature claims without file references
        claims_feature = bool(re.search(
            r'\b(?:module|feature|capability|function)\s+(?:is|has been)\s+(?:implemented|available|working)',
            response_lower
        ))
        has_file_ref = bool(re.search(r'\b[\w/]+\.py\b|\bsrc/|\bcore/|\bdetectors/', response_lower))
        
        if claims_feature and not has_file_ref:
            score += 0.2
            indicators.append("Claims feature without file/code reference")
        
        # 5. Future tense mixed with completion claims
        future_markers = len(re.findall(r'\bwill\s+(?:be|handle|process|enable)', response_lower))
        completion_markers = len(re.findall(r'\b(?:implemented|complete|working|ready)', response_lower))
        
        if future_markers > 0 and completion_markers > 0:
            score += 0.15
            indicators.append("Mixes future tense with completion claims")
        
        score = min(score, 1.0)
        detected = score >= 0.30  # Lowered threshold - this is a critical pattern
        
        explanation = ""
        if detected:
            if score > 0.6:
                explanation = "HIGH: Response likely describes intended design rather than verified implementation."
            else:
                explanation = "Warning: Response may conflate design plans with actual implementation."
        
        return BiasDetectionResult(
            bias_type='proposal_as_implementation',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_self_congratulatory(self, query: str, response: str) -> BiasDetectionResult:
        """
        Detect self-congratulatory bias - celebrating trivial achievements.
        
        Patterns:
        - Emojis and celebration markers for minor changes
        - Inflated improvement percentages
        - "SUCCESS" declarations for superficial fixes
        """
        indicators = []
        score = 0.0
        response_lower = response.lower()
        
        # 1. Celebration markers
        celebration_patterns = [
            r'ðŸŽ‰|ðŸŽŠ|âœ¨|ðŸš€|ðŸ’ª|ðŸ‘',  # Celebration emojis
            r'\bsuccess\s*!+',
            r'\bfully\s+working\s*!',
            r'\bgreat\s+(?:news|progress|improvement)',
            r'\b(?:amazing|excellent|perfect)\s+(?:result|outcome|improvement)',
        ]
        
        celebration_count = 0
        for pattern in celebration_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            celebration_count += len(matches)
        
        if celebration_count >= 2:
            score += 0.3
            indicators.append(f"Excessive celebration markers: {celebration_count}")
        
        # 2. Inflated improvement claims
        improvement_claims = re.findall(r'\+(\d+)\s*%', response)
        if improvement_claims:
            max_improvement = max(int(x) for x in improvement_claims)
            if max_improvement > 100:
                score += 0.3
                indicators.append(f"Suspicious improvement claim: +{max_improvement}%")
            elif max_improvement > 45:
                score += 0.15
                indicators.append(f"High improvement claim: +{max_improvement}%")
        
        # 3. Trivial fix + celebration mismatch
        trivial_fix_patterns = [
            r'changed?\s+["\']?\w+["\']?\s+to\s+["\']?\w+["\']?',  # Word replacement
            r'added?\s+(?:a\s+)?(?:word|comment|line)',
            r'(?:minor|small)\s+(?:change|update|fix)',
        ]
        
        has_trivial_fix = any(re.search(p, response_lower) for p in trivial_fix_patterns)
        has_celebration = celebration_count > 0
        
        if has_trivial_fix and has_celebration:
            score += 0.35
            indicators.append("Celebrates trivial change")
        
        # 4. Success declarations without evidence
        success_claims = len(re.findall(r'\b(?:success|fixed|resolved|complete)\s*[!âœ“âœ…]', response, re.IGNORECASE))
        evidence_markers = len(re.findall(r'\b(?:test|verify|output|result|shows?|proves?)\b', response_lower))
        
        if success_claims > evidence_markers:
            score += 0.2
            indicators.append(f"Success claims ({success_claims}) exceed evidence ({evidence_markers})")
        
        score = min(score, 1.0)
        detected = score > 0.3
        
        explanation = ""
        if detected:
            explanation = "Response may overstate the significance of changes made. Verify actual improvement."
        
        return BiasDetectionResult(
            bias_type='self_congratulatory',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_premature_closure(self, query: str, response: str) -> BiasDetectionResult:
        """
        Detect premature closure - declaring done without thorough verification.
        
        Patterns:
        - "Fixed" claims without testing multiple paths
        - Single test used to declare complete
        - Missing edge case consideration
        """
        indicators = []
        score = 0.0
        response_lower = response.lower()
        
        # 1. Single-path verification
        single_test_patterns = [
            r'tested?\s+(?:and\s+)?(?:works?|pass(?:es)?)',  # Generic "tested, works"
            r'verified?\s+(?:locally|in\s+(?:dev|test))',  # Single environment
            r'(?:the|this)\s+test\s+(?:pass(?:es)?|works?)',  # Single test reference
        ]
        
        single_path_indicators = 0
        for pattern in single_test_patterns:
            if re.search(pattern, response_lower):
                single_path_indicators += 1
        
        # Check for multi-path verification
        multi_path_markers = [
            r'(?:all|multiple|several)\s+(?:tests?|paths?|scenarios?)',
            r'(?:end[\s-]*to[\s-]*end|e2e|integration)\s+test',
            r'(?:edge\s+cases?|corner\s+cases?)',
        ]
        
        has_multi_path = any(re.search(p, response_lower) for p in multi_path_markers)
        
        if single_path_indicators > 0 and not has_multi_path:
            score += 0.25
            indicators.append("Single-path verification without comprehensive testing")
        
        # 2. Quick conclusion language
        quick_conclusion = [
            r'\bfix(?:ed)?\s+(?:and\s+)?(?:done|complete)',
            r'\bquick\s+(?:fix|update|change)',
            r'\bsimple\s+(?:fix|solution|change)',
            r'\bthat\s+(?:should|will)\s+(?:fix|solve)',
        ]
        
        for pattern in quick_conclusion:
            if re.search(pattern, response_lower):
                score += 0.15
                indicators.append(f"Quick conclusion language detected")
                break
        
        # 3. Missing verification steps
        claims_fixed = bool(re.search(r'\b(?:fixed|resolved|complete|done)\b', response_lower))
        
        verification_steps = [
            r'\b(?:run|execute)\s+(?:test|verify)',
            r'\b(?:output|result)\s*(?::|shows)',
            r'```(?:bash|shell|output)',  # Command output
        ]
        
        has_verification_output = any(re.search(p, response) for p in verification_steps)
        
        if claims_fixed and not has_verification_output:
            score += 0.25
            indicators.append("Claims fix without showing verification output")
        
        # 4. Assumptions about coverage
        assumption_patterns = [
            r'\bshould\s+(?:work|be\s+(?:fine|ok|good))',
            r'\bprobably\s+(?:fine|works?|fixed)',
            r'\bi\s+(?:think|believe)\s+(?:this|that|it)\s+(?:fix|work|resolv)',
        ]
        
        for pattern in assumption_patterns:
            if re.search(pattern, response_lower):
                score += 0.2
                indicators.append("Assumption-based conclusion")
                break
        
        score = min(score, 1.0)
        detected = score > 0.3
        
        explanation = ""
        if detected:
            explanation = "Response may conclude too quickly without comprehensive verification. Test all affected paths."
        
        return BiasDetectionResult(
            bias_type='premature_closure',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _detect_metric_gaming(self, 
                              response: str, 
                              history: List[Dict],
                              query: str = None) -> BiasDetectionResult:
        """
        Detect metric gaming.
        
        PPA 3, Invention 5:
        Response appears to game evaluation metrics.
        
        Phase 16 Enhancement: Context-aware detection to reduce false positives.
        Planning documents, technical docs, and time estimates are NOT gaming.
        """
        indicators = []
        score = 0.0
        
        # Phase 16: Context-aware detection
        context_adjustment = 1.0
        if CONTEXT_CLASSIFIER_AVAILABLE:
            context_signals = context_classifier.classify(response, query)
            
            # Check if we should skip metric gaming detection entirely
            should_skip, skip_reason = context_classifier.should_skip_metric_gaming(response, query)
            if should_skip:
                return BiasDetectionResult(
                    bias_type='metric_gaming',
                    detected=False,
                    confidence=0.0,
                    score=0.0,
                    indicators=[f"Skipped: {skip_reason}"],
                    explanation=f"Context-aware: {skip_reason}"
                )
            
            # Get adjustment factor for this context
            context_adjustment = context_signals.metric_gaming_adjustment
        
        # 1. Keyword stuffing
        words = response.lower().split()
        if words:
            word_freq = Counter(words)
            top_freq = word_freq.most_common(1)[0][1] if word_freq else 0
            
            if top_freq > 8:
                indicators.append(f"Keyword repetition: {top_freq}x")
                score += 0.3
        
        # 2. Response padding
        sentences = re.split(r'[.!?]+', response)
        if sentences:
            avg_sentence_len = sum(len(s.split()) for s in sentences) / len(sentences)
            
            if len(response) > 2000 and avg_sentence_len < 12:
                indicators.append(f"Response padding detected (long but short sentences)")
                score += 0.25
        
        # 3. Excessive hedging
        hedges = ['possibly', 'perhaps', 'might', 'could', 'may', 'sometimes', 
                  'in some cases', 'it depends', 'generally', 'typically']
        hedge_count = sum(1 for h in hedges if h in response.lower())
        
        if hedge_count > 4:
            indicators.append(f"Excessive hedging: {hedge_count} hedge words")
            score += 0.25
        
        # 4. Check for metric inflation vs holdout
        if history:
            recent_with_holdout = [h for h in history[-20:] if 'holdout_accuracy' in h]
            if recent_with_holdout:
                avg_inflation = sum(
                    h.get('observed_accuracy', 0) - h.get('holdout_accuracy', 0)
                    for h in recent_with_holdout
                ) / len(recent_with_holdout)
                
                if avg_inflation > 10:
                    indicators.append(f"Metric inflation detected: +{avg_inflation:.0f}% vs holdout")
                    score += 0.4
        
        # 5. Formulaic structure (gaming readability metrics)
        if re.search(r'^First,.*Second,.*Third,', response, re.I | re.S):
            indicators.append("Formulaic structure")
            score += 0.1
        
        # 6. Artificial comprehensiveness
        list_markers = re.findall(r'^\s*(?:\d+\.|[-â€¢*])\s*', response, re.M)
        if len(list_markers) > 10:
            indicators.append(f"Excessive listing: {len(list_markers)} items")
            score += 0.15
        
        # 7. Inflated comparison claims (Phase 7)
        response_lower = response.lower()
        
        # Large percentage improvements without evidence
        inflated_comparisons = re.findall(r'(\d{3,})%\s*(more|better|improvement|increase)', response_lower)
        if inflated_comparisons:
            score += 0.35
            indicators.append(f"Inflated comparison: {inflated_comparisons[0][0]}% {inflated_comparisons[0][1]}")
        
        # Issue/error count comparisons
        issue_counts = re.findall(r'(?:found|detected|identified)\s*[:\s]*(\d+)\s*(?:issue|error|problem)', response_lower)
        if len(issue_counts) >= 2:
            counts = [int(c) for c in issue_counts]
            if max(counts) / (min(counts) + 1) > 5:  # Ratio > 5:1
                score += 0.3
                indicators.append(f"Suspicious issue count ratio: {max(counts)} vs {min(counts)}")
        
        # Winner declarations without methodology
        if re.search(r'\bwinner\s*[:\s]', response_lower):
            if not re.search(r'\b(methodology|criteria|because|due\s+to|based\s+on)\b', response_lower):
                score += 0.25
                indicators.append("Winner declared without methodology explanation")
        
        # Unqualified superlatives in comparisons
        superlatives = len(re.findall(r'\b(best|worst|most|least|highest|lowest)\b', response_lower))
        if superlatives >= 2 and 'compar' in response_lower:
            score += 0.15
            indicators.append(f"Unqualified superlatives in comparison: {superlatives}")
        
        score = min(score, 1.0)
        
        # Phase 16: Apply context adjustment to reduce false positives
        # In planning/documentation contexts, we're more lenient
        if CONTEXT_CLASSIFIER_AVAILABLE and context_adjustment < 1.0:
            original_score = score
            score = score * context_adjustment
            if original_score != score:
                indicators.append(f"Context adjustment: {original_score:.2f} â†’ {score:.2f} ({context_adjustment:.0%})")
        
        detected = score >= 0.30  # Lowered threshold
        
        explanation = ""
        if detected:
            explanation = "Response may be optimizing for metrics rather than genuine helpfulness."
        elif CONTEXT_CLASSIFIER_AVAILABLE and context_adjustment < 1.0:
            explanation = "Context-aware: Legitimate planning/documentation content detected."
        
        return BiasDetectionResult(
            bias_type='metric_gaming',
            detected=detected,
            confidence=score,
            score=score,
            indicators=indicators,
            explanation=explanation
        )
    
    def _extract_expectation(self, query: str) -> Optional[str]:
        """Extract embedded expectation from query."""
        # Patterns that indicate what user expects
        patterns = [
            r"(?:confirm|prove|show|verify)\s+(?:that\s+)?(.+?)(?:\?|$)",
            r"isn't\s+it\s+(?:true|correct|obvious)\s+that\s+(.+?)(?:\?|$)",
            r"I\s+(?:think|believe|know)\s+(?:that\s+)?(.+?)(?:,|\.|\?|$)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query, re.I)
            if match:
                return match.group(1).strip()
        
        return None
    
    def _compute_alignment(self, response: str, expectation: str) -> float:
        """Compute how much response aligns with expectation."""
        exp_words = set(re.findall(r'\b\w{4,}\b', expectation.lower()))
        resp_words = set(re.findall(r'\b\w{4,}\b', response.lower()))
        
        if not exp_words:
            return 0.0
        
        overlap = len(exp_words & resp_words)
        return overlap / len(exp_words)
    
    def _simple_sentiment(self, text: str) -> float:
        """Simple sentiment scoring (-1 to 1)."""
        positive_words = {
            'great', 'excellent', 'good', 'wonderful', 'fantastic', 'amazing',
            'perfect', 'love', 'happy', 'best', 'definitely', 'absolutely',
            'certainly', 'correct', 'right', 'yes', 'agree', 'helpful'
        }
        negative_words = {
            'bad', 'terrible', 'awful', 'wrong', 'incorrect', 'no', 'never',
            'cannot', 'unable', 'unfortunately', 'sadly', 'disagree', 'error',
            'mistake', 'problem', 'issue', 'fail', 'failed', 'false'
        }
        
        words = set(re.findall(r'\b\w+\b', text.lower()))
        
        pos_count = len(words & positive_words)
        neg_count = len(words & negative_words)
        total = pos_count + neg_count
        
        if total == 0:
            return 0.0
        
        return (pos_count - neg_count) / total
    
    def _check_query_pattern(self, query: str) -> float:
        """Check if query follows a repeated pattern."""
        query_words = set(re.findall(r'\b\w{4,}\b', query.lower()))
        
        similarities = []
        for past in self.query_history:
            past_words = set(re.findall(r'\b\w{4,}\b', past['query'].lower()))
            if query_words and past_words:
                overlap = len(query_words & past_words) / max(len(query_words), len(past_words))
                similarities.append(overlap)
        
        return max(similarities) if similarities else 0.0
    
    def _generate_recommendations(self, 
                                  confirmation: BiasDetectionResult,
                                  reward: BiasDetectionResult,
                                  social: BiasDetectionResult,
                                  gaming: BiasDetectionResult) -> List[str]:
        """Generate actionable recommendations based on detected biases."""
        recommendations = []
        
        if confirmation.detected:
            recommendations.append(
                "Consider rephrasing query to seek objective information rather than confirmation."
            )
        
        if reward.detected:
            recommendations.append(
                "Response may prioritize satisfaction over accuracy. Verify facts independently."
            )
        
        if social.detected:
            recommendations.append(
                "Consider seeking diverse perspectives to avoid echo chamber effects."
            )
        
        if gaming.detected:
            recommendations.append(
                "Response structure suggests metric optimization. Focus on content quality."
            )
        
        return recommendations
    
    def record_feedback(self, 
                        bias_type: str, 
                        pattern: str, 
                        was_correct: bool):
        """Record feedback to improve pattern detection."""
        self.pattern_matcher.learn(pattern, bias_type, was_correct)
    
    def record_satisfaction_feedback(self, 
                                    decision_id: str,
                                    user_satisfaction: float,
                                    actual_accuracy: float,
                                    response_snippet: str = ""):
        """
        Record satisfaction vs accuracy feedback for reinforcement detection.
        
        PPA-3 Invention 2: Reinforcement Impairment Detection
        Track when user satisfaction diverges from actual accuracy.
        """
        self.reinforcement_tracker.record_outcome(
            decision_id, user_satisfaction, actual_accuracy, response_snippet
        )
    
    # Standard Learning Interface
    def adapt_thresholds(self, threshold_name, current_value, direction='decrease'):
        """Adapt thresholds based on learning history."""
        adjustment = 0.05 if direction == 'increase' else -0.05
        return max(0.1, min(0.95, current_value + adjustment))

    # =========================================================================
    # Learning Interface (5/5 methods) - Completing interface
    # =========================================================================
    
    def learn_from_feedback(self, feedback: Dict) -> None:
        """Learn from feedback to adjust behavior."""
        if not hasattr(self, '_learning_params'):
            self._learning_params = {}
        was_correct = feedback.get('was_correct', True)
        if not was_correct:
            adjustment = feedback.get('adjustment', 0.05)
            self._learning_params['threshold'] = self._learning_params.get('threshold', 0.5) * (1 - adjustment)
    
    def get_statistics(self) -> Dict:
        """Return learning statistics."""
        return {
            'total_operations': getattr(self, '_total_operations', 0),
            'learning_params': getattr(self, '_learning_params', {}),
            'history_size': len(getattr(self, '_outcomes', []))
        }
    
    def serialize_state(self) -> Dict:
        """Serialize learning state for persistence."""
        return {
            'outcomes': getattr(self, '_outcomes', [])[-100:],
            'learning_params': getattr(self, '_learning_params', {}),
            'version': '1.0'
        }
    
    def deserialize_state(self, state: Dict) -> None:
        """Restore learning state from serialized data."""
        self._outcomes = state.get('outcomes', [])
        self._learning_params = state.get('learning_params', {})

    def record_outcome_standard(self, outcome: Dict) -> None:
        """Standard learning interface wrapper."""
        if not hasattr(self, '_outcomes'):
            self._outcomes = []
        self._outcomes.append(outcome)
